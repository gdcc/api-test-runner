name: S3

on: workflow_dispatch

jobs:
  api:
    runs-on: ubuntu-latest
    steps:
      - name: Clone this repo
        uses: actions/checkout@v3
      - name: Set up JDK 17
        uses: actions/setup-java@v3
        with:
          java-version: '17'
          distribution: 'temurin'
      - name: Clone Dataverse repo
        uses: actions/checkout@v3
        with:
          repository: IQSS/dataverse
          path: dataverse
          ref: 6783-s3-tests
      - name: Start containers
        run: |
          docker compose -f docker-compose-s3.yml --env-file .env.s3 up -d
      - uses: emilioschepis/wait-for-endpoint@v1.0.3
        with:
          url: http://localhost:8080/api/info/version
          timeout: 120000
          interval: 10000
      - name: Test metadatablocks
        run: |
          curl localhost:8080/api/metadatablocks
      - uses: emilioschepis/wait-for-endpoint@v1.0.3
        with:
          url: http://localhost:8080/api/metadatablocks
          timeout: 120000
          interval: 10000
      - name: Put SUSHI config file in place
        run: |
          docker ps
          # When tests are run on localhost, Dataverse can access /tmp/sushi_sample_logs.json
          # See https://github.com/IQSS/dataverse/blob/develop/src/test/java/edu/harvard/iq/dataverse/api/MakeDataCountApiIT.java#L56
          docker exec test_dataverse sh -c 'curl https://raw.githubusercontent.com/IQSS/dataverse/develop/src/test/java/edu/harvard/iq/dataverse/makedatacount/sushi_sample_logs.json > /tmp/sushi_sample_logs.json && head /tmp/sushi_sample_logs.json'
      - name: Test Search API
        run: |
          sleep 30
          curl 'http://localhost:8080/api/search?q=*'
      - name: Test Solr
        run: |
          curl http://localhost:8983/solr/collection1/schema/fields
          curl 'http://localhost:8983/solr/collection1/select?q=*%3A*&wt=json&indent=true'
      - name: Run database sequence
        run: |
          cd dataverse
          export PGPASSWORD=secret
          # use correct database name
          sed -i 's/dvnapp/dataverse/g' doc/sphinx-guides/source/_static/util/createsequence.sql
          psql -h localhost -U dataverse dataverse -f doc/sphinx-guides/source/_static/util/createsequence.sql
      - name: Run API tests
        run: |
          cd dataverse
          INTEGRATION_TESTS=$(<tests/integration-tests.txt)
          # TODO: re-enable full test suite
          #mvn test -Dtest=$INTEGRATION_TESTS -Ddataverse.test.baseurl=http://localhost:8080
          mvn test -Dtest=S3AccessIT -Ddataverse.test.baseurl=http://localhost:8080
      - name: Dump docker logs on failure
        if: failure()
        uses: jwalton/gh-docker-logs@v2
        with:
          dest: './logs'
      - name: Tar logs
        if: failure()
        run: tar cvzf ./logs.tgz ./logs
      - name: Upload logs to GitHub
        if: failure()
        uses: actions/upload-artifact@master
        with:
          name: logs.tgz
          path: ./logs.tgz
      - name: Stop containers
        if: always()
        run: docker compose -f docker-compose-s3.yml --env-file .env.s3 down
